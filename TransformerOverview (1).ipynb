{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def softmax(x):\n",
        "     return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "list1 = [2.0, 1.0, 1.0]\n",
        "print(np.exp(list1))\n",
        "print(softmax(list1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It2PV3S4Dm6P",
        "outputId": "3b10ca7b-7cb3-4af2-e550-1de8a6fcf3f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7.3890561  2.71828183 2.71828183]\n",
            "[0.57611688 0.21194156 0.21194156]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(np.exp(list1), axis=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYt_AXt_EBKK",
        "outputId": "e0f0d091-b71a-4436-f2a2-0d456fdf142b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.82561975584874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Functions Description\n",
        "\n",
        "## softmax(x)\n",
        "\n",
        "This function computes the softmax activation function over the input array `x`.\n",
        "\n",
        "- It exponentiates each element of `x`.\n",
        "- It sums along the last axis.\n",
        "- It divides each element by the sum to normalize the values into a probability distribution.\n",
        "- The transpose operations ensure correct broadcasting and alignment of dimensions.\n",
        "\n",
        "## scaleddotproduct(q, k, v, mask=None)\n",
        "\n",
        "This function computes the scaled dot-product attention mechanism commonly used in transformer-based architectures.\n",
        "\n",
        "- It takes in three inputs: query (`q`), key (`k`), and value (`v`) matrices, along with an optional mask matrix (`mask`).\n",
        "- It calculates the dot product of `q` and the transpose of `k`.\n",
        "- It scales the result by the square root of the dimension of `k`.\n",
        "- It applies the softmax function to obtain attention scores.\n",
        "- If a mask is provided, it adds the mask to the scaled dot product.\n",
        "- Finally, it computes the weighted sum of `v` based on the attention scores and returns the result along with the attention scores.\n"
      ],
      "metadata": {
        "id": "Fg1P6SQ1ZKpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L, dk and dv are having dimension query\n",
        "L,dk,dv = 4,8,8\n",
        "q = np.random.randn(L,dk)\n",
        "k = np.random.randn(L,dk)\n",
        "v = np.random.randn(L,dv)\n",
        "print(q)\n",
        "print(k)\n",
        "print(v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ep29dg1Iwli",
        "outputId": "de376c32-f997-4756-c0fb-0929d28f9132"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.05077099 -0.13968095  0.28794113 -0.11155697  0.75073963 -0.52329682\n",
            "   1.04938617  1.04672942]\n",
            " [-0.54786664 -0.50825837 -0.69942501 -0.6401618   1.39375713 -1.50627131\n",
            "  -0.30578947 -0.5676279 ]\n",
            " [ 0.64067017  0.79348516 -1.3860305   0.06075065  0.56191492  0.03487981\n",
            "   0.26233773  0.7136653 ]\n",
            " [ 0.06370629  0.28620743  1.64315643  1.30902162  1.37355287  0.73041375\n",
            "  -1.53309107 -0.79906111]]\n",
            "[[-0.86570656  0.78788674 -1.80806456 -0.92054313  0.17036489 -0.30346231\n",
            "   0.64347536  0.93605272]\n",
            " [-0.52105998 -0.0313016   1.34147804 -2.25087616 -1.35403948 -0.65355577\n",
            "  -1.1342191  -0.36495378]\n",
            " [ 1.61557539 -0.96355223 -1.31860745 -0.88487251  0.82636694 -0.25747757\n",
            "  -0.43899771 -0.29155016]\n",
            " [ 1.91897767  0.99055125 -0.36616923 -0.32582732  0.45342366 -0.79588718\n",
            "  -0.8223103  -1.32627147]]\n",
            "[[ 1.88377581  1.74823426  1.19604261  1.28779537 -0.93109576  0.13238352\n",
            "  -1.21585038  0.89339695]\n",
            " [-0.5080888  -0.84069984  0.3165722   0.28480814 -0.13819108  0.18558206\n",
            "   0.24996386  0.01012336]\n",
            " [ 0.69496643  1.24666421  1.05673329 -0.18741488  1.22627554 -0.32799409\n",
            "  -0.60416164 -1.0460332 ]\n",
            " [-0.70884565 -0.20429162 -1.36504952  2.17720109  1.19549895 -1.62924881\n",
            "   0.53087437  0.44417438]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0KHHnrz39fF_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "     return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "def scaleddotproduct(q,k,v,mask=None):\n",
        "      dk = q.shape[-1]\n",
        "      scaled = np.matmul(q, k.T) / np.sqrt(dk)\n",
        "      if mask is not None:\n",
        "           scaled = scaled + mask\n",
        "      print(scaled)\n",
        "      attention = softmax(scaled)\n",
        "      print(attention)\n",
        "      out = np.matmul(attention, v)\n",
        "      return out, attention\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values, attention = scaleddotproduct(q, k, v, mask=None)\n",
        "print(values)\n",
        "print(attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4a3PbpfHMrt",
        "outputId": "dd4b6195-1b3f-43ed-eace-1691d6a367b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.51538378 -0.55811038 -0.08454548 -0.63609788]\n",
            " [ 0.66969732  0.16095966  1.03685225  0.61693558]\n",
            " [ 1.21715534 -1.30687151  0.76950626  0.55434879]\n",
            " [-2.02505242 -0.38575158 -0.58154784  0.61500213]]\n",
            "[[0.45313856 0.15488823 0.24870531 0.1432679 ]\n",
            " [0.25040806 0.15055887 0.36149448 0.23753859]\n",
            " [0.44749377 0.03586048 0.28600621 0.23063954]\n",
            " [0.04098185 0.21112071 0.17357916 0.57431828]]\n",
            "[[ 0.8462015   0.94276141  0.65825374  0.89297512  0.03293829 -0.22626037\n",
            "  -0.58643318  0.20988252]\n",
            " [ 0.47806371  0.71333222  0.40491278  0.81477456  0.47330919 -0.44448659\n",
            "  -0.35912241 -0.04738871]\n",
            " [ 0.86003452  1.06161203  0.53397194  1.0350406   0.2048366  -0.40368168\n",
            "  -0.58547501  0.20342478]\n",
            " [-0.31653878 -0.00677676 -0.48469504  1.3307802   0.83211977 -0.94803479\n",
            "   0.20296573  0.1122782 ]]\n",
            "[[0.45313856 0.15488823 0.24870531 0.1432679 ]\n",
            " [0.25040806 0.15055887 0.36149448 0.23753859]\n",
            " [0.44749377 0.03586048 0.28600621 0.23063954]\n",
            " [0.04098185 0.21112071 0.17357916 0.57431828]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.tril(np.ones((L,L)))\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIc2Lt1tQDD8",
        "outputId": "a5b02b41-da95-4a39-8b3b-b35a46e1bb5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [1. 1. 0. 0.]\n",
            " [1. 1. 1. 0.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask[mask==0]=-np.inf\n",
        "mask[mask==1]=0\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3r-q_lYQJli",
        "outputId": "05285d27-605e-4fdd-eb85-4ebf7c174e9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0. -inf -inf -inf]\n",
            " [  0.   0. -inf -inf]\n",
            " [  0.   0.   0. -inf]\n",
            " [  0.   0.   0.   0.]]\n"
          ]
        }
      ]
    }
  ]
}