{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p3NB4XRudm-e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sequence_length = 4\n",
        "batch_size = 1\n",
        "input_dim = 512\n",
        "d_model = 512\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((batch_size, sequence_length, input_dim))\n"
      ],
      "metadata": {
        "id": "Y3HuAfJ7eG_5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU1aXz04eIds",
        "outputId": "7a7455c5-a361-4f8b-f15b-c684b403e4ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're using PyTorch's `nn.Linear` module to define a linear transformation layer, often called a \"fully connected layer\" or \"dense layer.\" Here's a breakdown of what's happening:\n",
        "\n",
        "- `nn.Linear(input_dim, 3 * d_model)`:\n",
        "  - This line creates an instance of the `nn.Linear` module.\n",
        "  - `input_dim` represents the size of each input sample (the last dimension of `x`).\n",
        "  - `3 * d_model` represents the output size of the linear layer. Here, we're using `3 * d_model` becausme in transformer architectures, the input is split into queries, keys, and values. Each of these components gets its own linear transformation, hence the factor of 3.\n",
        "\n",
        "- `qkv_layer(x)`:\n",
        "  - This line applies the linear transformation defined by `qkv_layer` to the input tensor `x`.\n",
        "  - The input tensor `x` has a shape of `(batch_size, sequence_length, input_dim)`.\n",
        "  - The output tensor `qkv` will have a shape\n",
        "  of `(batch_size, sequence_length, 3 * d_model)`, as each input sample is transformed into three separate parts: queries, keys, and values, each with a dimensionality of `d_model`.\n",
        "\n",
        "So, essentially, `qkv_layer` is used to linearly transform the input tensor `x` into a tensor `qkv` where each input sample is split into queries, keys, and values for further processing in self-attention mechanism."
      ],
      "metadata": {
        "id": "MQ4dwuHfeO5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qkv_layer = nn.Linear(input_dim, 3 * d_model)\n",
        "qkv = qkv_layer(x)\n"
      ],
      "metadata": {
        "id": "xN7mWSeYfubL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
        "x_val = np.arange(-1, 1, 0.01) * 3\n",
        "plt.bar(x_val, y_val, align='center', color=['forestgreen'])\n",
        "plt.title('qkv distribution')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "aRYcE17ufzbc",
        "outputId": "95845f73-1bf9-4501-f01d-adef1b40ead3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'qkv distribution')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In transformer-based architectures, such as the Transformer model used in natural language processing tasks, multi-head attention is a key component. This operation is designed to capture different aspects of the input sequence in parallel. The `num_heads` parameter controls how many heads are used in the multi-head attention mechanism.\n",
        "\n",
        "Here's a detailed explanation:\n",
        "\n",
        "- **`num_heads = 8`:**\n",
        "  - This line defines the number of attention heads. Each head is a separate attention mechanism that operates in parallel.\n",
        "\n",
        "- **`head_dim = d_model // num_heads`:**\n",
        "  - This line calculates the dimensionality of each head. It divides the total model dimension (`d_model`) by the number of heads (`num_heads`).\n",
        "  - For example, if `d_model = 512` and `num_heads = 8`, then `head_dim` would be `512 // 8 = 64`. This means each head will have a dimensionality of 64.\n",
        "\n",
        "- **`qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)`:**\n",
        "  - This line reshapes the `qkv` tensor to split it into `num_heads` separate heads.\n",
        "  - The `reshape` function is used to reshape the tensor while preserving its total number of elements.\n",
        "  - The shape of the tensor is changed to `(batch_size, sequence_length, num_heads, 3 * head_dim)`.\n",
        "    - `batch_size`: The number of samples in the batch.\n",
        "    - `sequence_length`: The length of each sequence.\n",
        "    - `num_heads`: The number of attention heads.\n",
        "    - `3 * head_dim`: The total dimensionality of the queries, keys, and values concatenated together for each head.\n",
        "  - This reshaping operation essentially splits the tensor along its third dimension into multiple heads, allowing each head to perform attention independently.\n",
        "\n",
        "In summary, this code prepares the `qkv` tensor for multi-head attention by reshaping it to have a separate dimension for each attention head, along with the concatenated queries, keys, and values for each head. This facilitates parallel processing of the input sequence by multiple attention mechanisms.\n"
      ],
      "metadata": {
        "id": "nSVDbA6012k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 8\n",
        "head_dim = d_model // num_heads\n",
        "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
        "print(qkv.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpptZwko0zYV",
        "outputId": "070708d3-1820-43dc-8813-1b8ab8cf49c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 8, 192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's break down the code step by step:\n",
        "\n",
        "**`qkv = qkv.permute(0, 2, 1, 3):`**\n",
        "\n",
        "- This line permutes the dimensions of the `qkv` tensor.\n",
        "- The parameters `(0, 2, 1, 3)` indicate the new order of dimensions after permutation:\n",
        "  - `0`: Represents the batch dimension, which remains unchanged.\n",
        "  - `2` and `1`: Swap the positions of the second and third dimensions.\n",
        "  - `3`: Represents the last dimension, which also remains unchanged.\n",
        "- The effect of this permutation is to swap the dimensions corresponding to the sequence length and the number of heads. After this operation, the tensor shape becomes `(batch_size, num_heads, sequence_length, 3 * head_dim)`.\n",
        "- This permutation is necessary to align the dimensions properly for further processing in multi-head attention. Specifically, it ensures that each head operates over the sequence length dimension.\n",
        "\n",
        "**`q, k, v = qkv.chunk(3, dim=-1):`**\n",
        "\n",
        "- This line splits the `qkv` tensor into three separate tensors: queries (`q`), keys (`k`), and values (`v`), along the last dimension (dimension `-1`).\n",
        "- The `chunk` function divides the tensor into a specified number of chunks along a given dimension.\n",
        "- Here, we split the tensor into three chunks because the `qkv` tensor contains concatenated queries, keys, and values for each head.\n",
        "- After splitting, each resulting tensor (`q`, `k`, `v`) contains the corresponding queries, keys, and values for all heads.\n",
        "- The resulting shapes of `q`, `k`, and `v` tensors are `(batch_size, num_heads, sequence_length, head_dim)`.\n",
        "\n",
        "In summary, these operations prepare the `q`, `k`, and `v` tensors for use in the multi-head attention mechanism by properly arranging their dimensions and splitting them into separate components for each head. This ensures that each head operates independently over the input sequence while processing queries, keys, and values.\n"
      ],
      "metadata": {
        "id": "GykleMud_UzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qkv = qkv.permute(0, 2, 1, 3)\n",
        "q, k, v = qkv.chunk(3, dim=-1)\n"
      ],
      "metadata": {
        "id": "s6tMihEd_mEr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.shape, k.shape, v.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Fe4aGP_5On",
        "outputId": "569c92c7-6552-4ca4-c5b5-a46a0fb798f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 8, 4, 64]),\n",
              " torch.Size([1, 8, 4, 64]),\n",
              " torch.Size([1, 8, 4, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Attention Scores (`scaled`):\n",
        "\n",
        "```python\n",
        "# Extracting the size of the last dimension of the query tensor\n",
        "d_k = q.size()[-1]\n",
        "# Computing dot products of queries and keys, scaled by square root of d_k\n",
        "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n"
      ],
      "metadata": {
        "id": "TOV_h1EUDXPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the size of the last dimension of the query tensor\n",
        "import math\n",
        "d_k = q.size()[-1]\n",
        "# Computing dot products of queries and keys, scaled by square root of d_k\n",
        "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n"
      ],
      "metadata": {
        "id": "31t8eQJkDi4v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a mask tensor with negative infinity values\n",
        "mask = torch.full(scaled.size(), float('-inf'))\n",
        "# Creating an upper triangular matrix mask to prevent attending to future tokens\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "# Applying the mask to the attention scores for a single head and a single input sample\n",
        "(scaled + mask)[0][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpbApYS7FfYh",
        "outputId": "3db27274-753f-47f2-c0a5-0a9811c38ad6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3353,    -inf,    -inf,    -inf],\n",
              "        [-0.4087,  0.4564,    -inf,    -inf],\n",
              "        [-0.1196, -0.3300,  0.1727,    -inf],\n",
              "        [ 0.0363,  0.2193, -0.0067,  0.2410]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying softmax activation along the last dimension of the attention scores\n",
        "attention = F.softmax(scaled, dim=-1)\n",
        "# Retrieving the shape of the attention tensor after applying softmax\n",
        "attention.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng_r4NmLF8j9",
        "outputId": "c3763b1e-b8a9-465e-8398-fc82662c1005"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the weighted sum of values using attention weights and value tensor\n",
        "values = torch.matmul(attention, v)\n"
      ],
      "metadata": {
        "id": "mhcfCpPmGWAW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzPId4bcGXob",
        "outputId": "c93eced6-5713-44ff-bb0a-f8664c6c5c96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.0946, -0.2888, -0.1424,  ...,  0.0511,  0.3030,  0.1836],\n",
            "          [ 0.0890, -0.3701,  0.0626,  ..., -0.0285,  0.1979, -0.0579],\n",
            "          [-0.0686, -0.2867, -0.1100,  ..., -0.0317,  0.2708,  0.1584],\n",
            "          [ 0.0026, -0.3245, -0.0118,  ...,  0.0191,  0.2281,  0.0745]],\n",
            "\n",
            "         [[-0.2998,  0.1964, -0.0473,  ..., -0.1905,  0.1021, -0.2069],\n",
            "          [-0.4986,  0.4261, -0.1545,  ...,  0.0674,  0.1723, -0.0168],\n",
            "          [-0.4851,  0.4021, -0.0811,  ...,  0.0688,  0.1441,  0.0230],\n",
            "          [-0.3431,  0.2578, -0.1447,  ..., -0.1540,  0.1357, -0.2265]],\n",
            "\n",
            "         [[ 0.1513,  0.0773,  0.2384,  ..., -0.0008, -0.3715,  0.2947],\n",
            "          [ 0.3013, -0.0193,  0.3360,  ..., -0.0863, -0.4622,  0.2862],\n",
            "          [ 0.3249,  0.1756,  0.2287,  ...,  0.0438, -0.5490,  0.2743],\n",
            "          [ 0.1015,  0.1486,  0.2029,  ..., -0.0248, -0.5183,  0.2964]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1544, -0.4101, -0.0822,  ..., -0.1144,  0.2886, -0.0138],\n",
            "          [-0.1104, -0.3622,  0.0244,  ..., -0.0775,  0.3402,  0.0185],\n",
            "          [-0.0926, -0.3911, -0.0102,  ..., -0.0867,  0.3734,  0.0150],\n",
            "          [-0.1188, -0.3938, -0.0468,  ..., -0.1021,  0.3507,  0.0010]],\n",
            "\n",
            "         [[ 0.3711,  0.2633,  0.2117,  ..., -0.0117, -0.1564,  0.1407],\n",
            "          [ 0.4281,  0.4869,  0.0248,  ...,  0.1921, -0.2462,  0.2874],\n",
            "          [ 0.4561,  0.4402,  0.0764,  ...,  0.0588, -0.1824,  0.2830],\n",
            "          [ 0.2808,  0.2483,  0.1948,  ...,  0.1712, -0.2388,  0.0758]],\n",
            "\n",
            "         [[-0.1677,  0.0420,  0.2938,  ..., -0.4165,  0.1335,  0.0124],\n",
            "          [-0.1179,  0.1330,  0.2491,  ..., -0.3126,  0.1511, -0.0740],\n",
            "          [-0.1962, -0.0447,  0.3013,  ..., -0.4705,  0.1126,  0.0524],\n",
            "          [-0.1649,  0.1576,  0.2788,  ..., -0.4331,  0.0944, -0.0667]]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muc4CO4zGhxn",
        "outputId": "bb5d0f34-4081-4493-e569-61ad2c84bbfb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Define the Scaled Dot-Product Attention Function:\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    # Calculate the dimension of the key vectors\n",
        "    d_k = q.size()[-1]\n",
        "    # Compute scaled dot-product attention scores\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    # Apply the mask (if provided)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    # Apply softmax activation to obtain attention weights\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    # Compute the weighted sum of values using attention weights\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n"
      ],
      "metadata": {
        "id": "_fWtNmQkII0M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiIXIBIAIQc3",
        "outputId": "d9f352b9-b1b6-4dde-a5f2-9d79afc935df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the Attention Weights for the First Head and Input Sample:"
      ],
      "metadata": {
        "id": "umWyoH6FInxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention[0][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjLRwnOFITMI",
        "outputId": "5daf3a7d-9341-49b2-840a-131c5f3a09a2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3344, 0.1861, 0.1813, 0.2981],\n",
              "        [0.1716, 0.4077, 0.2043, 0.2164],\n",
              "        [0.2400, 0.1945, 0.3215, 0.2440],\n",
              "        [0.2280, 0.2738, 0.2184, 0.2798]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the values tensor to prepare for linear transformation\n",
        "values = values.reshape(batch_size, sequence_length, num_heads * head_dim)\n"
      ],
      "metadata": {
        "id": "Pziu1_m6Ip3G"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply a linear transformation to the values tensor\n",
        "linear_layer = nn.Linear(d_model, d_model)\n",
        "out = linear_layer(values)\n"
      ],
      "metadata": {
        "id": "9kHVDtwqIykG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqj0pAV0I5Qp",
        "outputId": "adc63b44-c054-4d00-c93c-89c68ac9419b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2082, -0.1142,  0.1651,  ..., -0.1754, -0.2433, -0.1955],\n",
              "         [-0.3786, -0.0475, -0.0518,  ..., -0.0425,  0.0335, -0.2609],\n",
              "         [-0.3098,  0.1100, -0.0906,  ..., -0.4140,  0.0836, -0.0545],\n",
              "         [ 0.0743, -0.0946,  0.0593,  ..., -0.0288, -0.0335, -0.2902]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Certainly! Let's illustrate the process of computing the weighted sum using a simplified example with one head of the multi-head attention mechanism.\n",
        "\n",
        "# Suppose we have the following input sequence with three words (represented as vectors):\n",
        "\n",
        "# makefile\n",
        "# Copy code\n",
        "# Word1: [1, 2, 3]\n",
        "# Word2: [4, 5, 6]\n",
        "# Word3: [7, 8, 9]\n",
        "# For this example, let's assume we have one head with learned queries, keys, and values for simplicity. We'll also use a simple attention mechanism to compute attention scores.\n",
        "\n",
        "# Queries, Keys, and Values: Suppose our learned parameters for this head are as follows:\n",
        "\n",
        "# Queries: [0.1, 0.2, 0.3]\n",
        "\n",
        "# Keys: [0.4, 0.5, 0.6]\n",
        "\n",
        "# Values: [0.7, 0.8, 0.9]\n",
        "\n",
        "# Attention Scores: To compute attention scores, we first calculate the dot product between the queries and keys for each word:\n",
        "\n",
        "# Attention score for Word1: (0.1 * 0.4) + (0.2 * 0.5) + (0.3 * 0.6) = 1.4\n",
        "\n",
        "# Attention score for Word2: (0.1 * 0.4) + (0.2 * 0.5) + (0.3 * 0.6) = 3.2\n",
        "\n",
        "# Attention score for Word3: (0.1 * 0.4) + (0.2 * 0.5) + (0.3 * 0.6) = 5.0\n",
        "\n",
        "# Attention Weights: We normalize the attention scores using the softmax function to obtain attention weights:\n",
        "\n",
        "# Attention weight for Word1: softmax([1.4, 3.2, 5.0]) ≈ [0.02, 0.16, 0.82]\n",
        "\n",
        "# Attention weight for Word2: softmax([1.4, 3.2, 5.0]) ≈ [0.02, 0.16, 0.82]\n",
        "\n",
        "# Attention weight for Word3: softmax([1.4, 3.2, 5.0]) ≈ [0.02, 0.16, 0.82]\n",
        "\n",
        "# Weighted Sum: Finally, we compute the weighted sum of the values using the attention weights:\n",
        "\n",
        "# Weighted sum for Word1: (0.02 * 0.7) + (0.16 * 0.8) + (0.82 * 0.9) ≈ 0.746\n",
        "\n",
        "# Weighted sum for Word2: (0.02 * 0.7) + (0.16 * 0.8) + (0.82 * 0.9) ≈ 0.746\n",
        "\n",
        "# Weighted sum for Word3: (0.02 * 0.7) + (0.16 * 0.8) + (0.82 * 0.9) ≈ 0.746\n",
        "\n",
        "# These weighted sums represent the output of the attention mechanism for this particular head. They capture the importance of each word in the input sequence based on the learned attention scores and are used as input to subsequent layers of the neural network."
      ],
      "metadata": {
        "id": "ZL2u48bbrJMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention Mechanism\n",
        "\n",
        "#### Queries, Keys, and Values\n",
        "- Each head has its own learned parameters for queries, keys, and values.\n",
        "- These parameters are obtained through separate linear transformations of the input sequence.\n",
        "\n",
        "#### Attention Scores\n",
        "- Using the queries, keys, and values, each head independently computes attention scores for each word in the input sequence.\n",
        "- Attention scores represent the relevance or importance of each word with respect to the other words in the sequence.\n",
        "\n",
        "#### Weighted Sum\n",
        "- The attention scores are used to compute a weighted sum of the values.\n",
        "- The weights are determined by the attention scores.\n",
        "- This weighted sum represents the output of the attention mechanism for that particular head.\n",
        "\n",
        "#### Combination of Heads\n",
        "- The outputs of all heads are typically concatenated or combined to produce the final output of the multi-head attention mechanism.\n",
        "\n",
        "#### Benefits\n",
        "- Multiple heads enable the model to learn different patterns and relationships within the input sequence.\n",
        "- Each head's attention scores provide insights into how that head attends to different parts of the sequence, capturing diverse information and improving model performance.\n"
      ],
      "metadata": {
        "id": "Jk71crvZrfNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sure, let's clarify this with an example. Consider a simple sequence of word embeddings:\n",
        "\n",
        "# makefile\n",
        "# Copy code\n",
        "# Sequence: [word1, word2, word3, word4]\n",
        "# Now, let's say we have a multi-head attention mechanism with 2 heads. Each head will compute its own set of queries, keys, and values for each word in the sequence. Here's how it works:\n",
        "\n",
        "# Initialization: Each head has its own learnable parameters for the linear transformations that produce queries, keys, and values.\n",
        "\n",
        "# Processing the Input Sequence: For each head:\n",
        "\n",
        "# The input sequence is passed through separate linear transformations to compute queries, keys, and values.\n",
        "# These transformations are unique to each head, meaning they have their own sets of parameters.\n",
        "# For example, let's say the linear transformation for queries in Head 1 outputs [q1_word1, q1_word2, q1_word3, q1_word4], and the linear transformation for keys in Head 1 outputs [k1_word1, k1_word2, k1_word3, k1_word4].\n",
        "# Computing Attention Scores: Using the computed queries and keys, each head computes attention scores for each word in the sequence. These attention scores represent the importance of each word with respect to the others.\n",
        "\n",
        "# Weighted Sum: The attention scores are then used to compute a weighted sum of the values, where the weights are determined by the attention scores. This weighted sum represents the output of the attention mechanism for that particular head.\n",
        "\n",
        "# Combining Outputs of Multiple Heads: The outputs of all heads are typically combined in some way (e.g., concatenated or averaged) to produce the final output of the multi-head attention mechanism.\n",
        "\n",
        "# In summary, each head in the multi-head attention mechanism learns its own set of parameters for processing the input sequence, allowing it to capture different patterns and relationships within the data. Despite sharing the same input sequence, each head's computations are independent and result in unique representations of the input.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4SdriASwtbkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In your code, the initialization of learnable parameters for the linear transformations is implicitly done when you define the linear layers (`nn.Linear`) within the `MultiheadAttention` class. Here's where it happens:\n",
        "\n",
        "```python\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        \n",
        "        # Define linear layers for queries, keys, and values\n",
        "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)  # This initializes the parameters\n",
        "        \n",
        "        # Additional linear layer for output transformation\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "In the __init__ method of the MultiheadAttention class, you create a linear layer self.qkv_layer with input dimension input_dim and output dimension 3 * d_model. This layer contains the learnable parameters (weights and biases) that will be initialized with random values when the model is created. These parameters will then be updated during training through backpropagation. Similarly, you define another linear layer self.linear_layer for the output transformation.\n",
        "\n",
        "So, the initialization of learnable parameters happens when you instantiate the MultiheadAttention class, and it's handled internally by PyTorch when you create the linear layers."
      ],
      "metadata": {
        "id": "wZ25PSQYu3dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In your code, the initialization of learnable parameters for the linear transformations is implicitly done when you define the linear layers (nn.Linear) within the MultiheadAttention class. Here's where it happens:\n",
        "\n",
        "# python\n",
        "# Copy code\n",
        "# class MultiheadAttention(nn.Module):\n",
        "\n",
        "#     def __init__(self, input_dim, d_model, num_heads):\n",
        "#         super().__init__()\n",
        "#         self.input_dim = input_dim\n",
        "#         self.d_model = d_model\n",
        "#         self.num_heads = num_heads\n",
        "#         self.head_dim = d_model // num_heads\n",
        "\n",
        "#         # Define linear layers for queries, keys, and values\n",
        "#         self.qkv_layer = nn.Linear(input_dim , 3 * d_model)  # This initializes the parameters\n",
        "\n",
        "#         # Additional linear layer for output transformation\n",
        "#         self.linear_layer = nn.Linear(d_model, d_model)\n",
        "# In the __init__ method of the MultiheadAttention class, you create a linear layer self.qkv_layer with input dimension input_dim and output dimension 3 * d_model. This layer contains the learnable parameters (weights and biases) that will be initialized with random values when the model is created. These parameters will then be updated during training through backpropagation. Similarly, you define another linear layer self.linear_layer for the output transformation.\n",
        "\n",
        "# So, the initialization of learnable parameters happens when you instantiate the MultiheadAttention class, and it's handled internally by PyTorch when you create the linear layers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RR6p7IT0urOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Sequence\n",
        "Word1: [1, 2, 3]  \n",
        "Word2: [4, 5, 6]  \n",
        "Word3: [7, 8, 9]  \n",
        "\n",
        "### Learned Parameters\n",
        "Queries: [0.1, 0.2, 0.3]  \n",
        "Keys: [0.4, 0.5, 0.6]  \n",
        "Values: [0.7, 0.8, 0.9]  \n",
        "\n",
        "### Attention Scores\n",
        "- Attention score for Word1: 1.4  \n",
        "- Attention score for Word2: 3.2  \n",
        "- Attention score for Word3: 5.0  \n",
        "\n",
        "### Attention Weights\n",
        "- Attention weight for Word1: [0.02, 0.16, 0.82]  \n",
        "- Attention weight for Word2: [0.02, 0.16, 0.82]  \n",
        "- Attention weight for Word3: [0.02, 0.16, 0.82]  \n",
        "\n",
        "### Weighted Sum\n",
        "- Weighted sum for Word1: 0.746  \n",
        "- Weighted sum for Word2: 0.746  \n",
        "- Weighted sum for Word3: 0.746  \n"
      ],
      "metadata": {
        "id": "E_TJfmIXqstW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "8o7bYpcdMQTv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 1024\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "batch_size = 30\n",
        "sequence_length = 5\n",
        "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
        "\n",
        "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
        "out = model.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq6y7mv1MdX_",
        "outputId": "302ee146-1900-4a36-bad9-76fd0a846207"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.size(): torch.Size([30, 5, 1024])\n",
            "qkv.size(): torch.Size([30, 5, 1536])\n",
            "qkv.size(): torch.Size([30, 5, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 5, 192])\n",
            "q size: torch.Size([30, 8, 5, 64]), k size: torch.Size([30, 8, 5, 64]), v size: torch.Size([30, 8, 5, 64]), \n",
            "values.size(): torch.Size([30, 8, 5, 64]), attention.size:torch.Size([30, 8, 5, 5]) \n",
            "values.size(): torch.Size([30, 5, 512])\n",
            "out.size(): torch.Size([30, 5, 512])\n"
          ]
        }
      ]
    }
  ]
}